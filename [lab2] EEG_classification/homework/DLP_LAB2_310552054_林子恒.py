# -*- coding: utf-8 -*-
"""DLP_LAB2_310552054_林子恒.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1oazw7PL3xmDhu9-EL3Kjuw6_vbJ00Ll1
"""

!python -V
!nvcc --version
!nvidia-smi

# Commented out IPython magic to ensure Python compatibility.
#!pip3 install torch==1.7.1+cu110 torchvision==0.8.1+cu110 torchaudio==0.9.0 -f https://download.pytorch.org/whl/torch_stable.html
#!pip install tensorboard
# %load_ext tensorboard

import numpy as np
import random
import torch
import torch.nn as nn
from torch.optim import SGD
from torch.utils.data import DataLoader
from sklearn.model_selection import KFold
from torch.utils.tensorboard import SummaryWriter
# This is for the progress bar.
from tqdm.auto import tqdm
import datetime, os

print(torch.__version__)
print(torch.cuda.is_available())

# Commented out IPython magic to ensure Python compatibility.
from google.colab import drive
drive.mount('/content/drive')
# %cd drive/MyDrive/NYCU/"109 Summer Deep Learning and Practice"/"assignment"/"[lab2] EEG_classification"/
!ls

# Commented out IPython magic to ensure Python compatibility.
from tensorboard import notebook
notebook.list() # View open TensorBoard instances
# Control TensorBoard display. If no port is provided, 
# the most recently launched TensorBoard is used
'''notebook.display(port=6006, height=800)

notebook.display(port=6007, height=800)'''

# %tensorboard --logdir runs/

#To split the train set and valid set
#rate is train set rate
#valid set rate is 1 - rate
def split( train_set, rate):
  random.shuffle(train_set)
  pivot = np.int_(np.around(len(train_set)*rate))
  return train_set[:pivot] , train_set[pivot:]

import dataloader as dataloader
train_data, train_label, test_data, test_label = dataloader.read_bci_data()
train_data = np.float32(train_data)
train_label = np.int_(train_label)
test_data = np.float32(test_data)
test_label = np.int_(test_label)

train_set = []
valid_set = []
test_set = []

for i in range(len(train_data)):
  train_set.append([train_data[i],train_label[i]])
for i in range(len(test_data)):
  test_set.append([test_data[i],test_label[i]])

train_set , valid_set = split(train_set,0.8)

batch_size = 1024
train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)
valid_loader = DataLoader(valid_set, batch_size=batch_size, shuffle=True)
test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False)

class EGGNet_ReLU(nn.Module):
  def __init__(self):
    super( EGGNet_ReLU, self).__init__()
    self.firstconv = nn.Sequential(
        nn.Conv2d(1, 16, kernel_size=(1,51), stride=1, padding=(0,25) , bias=False),
        nn.BatchNorm2d(16)
    )
    self.depthwiseConv = torch.nn.Sequential(
        nn.Conv2d(16, 32, kernel_size=(2,1), stride=1, groups=16, bias=False),
        nn.BatchNorm2d(32),
        nn.ReLU(),
        nn.AvgPool2d(kernel_size=(1,4), stride=(1,4), padding=0),
        nn.Dropout(p=0.25)
    )
    self.separableConv = torch.nn.Sequential(
        nn.Conv2d(32, 32, kernel_size=(1,15), stride=1, padding=(0,7), bias=False),
        nn.BatchNorm2d(32),
        nn.ReLU(),
        nn.AvgPool2d(kernel_size=(1,8), stride=(1,8), padding=0),
        nn.Dropout(p=0.25)
    )
    self.classify = torch.nn.Sequential(
        nn.Linear(736, 2, bias=True)
    )

  def forward(self, x):
    x = self.firstconv(x)
    x = self.depthwiseConv(x)
    x = self.separableConv(x)
    x = x.flatten(1)
    x = self.classify(x)
    return x

class EGGNet_LeakyReLU(nn.Module):
  def __init__(self):
    super( EGGNet_LeakyReLU, self).__init__()
    self.firstconv = nn.Sequential(
        nn.Conv2d(1, 16, kernel_size=(1,51), stride=1, padding=(0,25) , bias=False),
        nn.BatchNorm2d(16)
    )
    self.depthwiseConv = torch.nn.Sequential(
        nn.Conv2d(16, 32, kernel_size=(2,1), stride=1, groups=16, bias=False),
        nn.BatchNorm2d(32),
        nn.LeakyReLU(),
        nn.AvgPool2d(kernel_size=(1,4), stride=(1,4), padding=0),
        nn.Dropout(p=0.25)
    )
    self.separableConv = torch.nn.Sequential(
        nn.Conv2d(32, 32, kernel_size=(1,15), stride=1, padding=(0,7), bias=False),
        nn.BatchNorm2d(32),
        nn.LeakyReLU(),
        nn.AvgPool2d(kernel_size=(1,8), stride=(1,8), padding=0),
        nn.Dropout(p=0.25)
    )
    self.classify = torch.nn.Sequential(
        nn.Linear(736, 2, bias=True)
    )

  def forward(self, x):
    x = self.firstconv(x)
    x = self.depthwiseConv(x)
    x = self.separableConv(x)
    x = x.flatten(1)
    x = self.classify(x)
    return x

class EGGNet_ELU(nn.Module):
  def __init__(self):
    super( EGGNet_ELU, self).__init__()
    self.firstconv = nn.Sequential(
        nn.Conv2d(1, 16, kernel_size=(1,51), stride=1, padding=(0,25) , bias=False),
        nn.BatchNorm2d(16)
    )
    self.depthwiseConv = torch.nn.Sequential(
        nn.Conv2d(16, 32, kernel_size=(2,1), stride=1, groups=16, bias=False),
        nn.BatchNorm2d(32),
        nn.ELU(),
        nn.AvgPool2d(kernel_size=(1,4), stride=(1,4), padding=0),
        nn.Dropout(p=0.25)
    )
    self.separableConv = torch.nn.Sequential(
        nn.Conv2d(32, 32, kernel_size=(1,15), stride=1, padding=(0,7), bias=False),
        nn.BatchNorm2d(32),
        nn.ELU(),
        nn.AvgPool2d(kernel_size=(1,8), stride=(1,8), padding=0),
        nn.Dropout(p=0.25)
    )
    self.classify = torch.nn.Sequential(
        nn.Linear(736, 2, bias=True)
    )

  def forward(self, x):
    x = self.firstconv(x)
    x = self.depthwiseConv(x)
    x = self.separableConv(x)
    x = x.flatten(1)
    x = self.classify(x)
    return x

class DeepConvNet_Relu(nn.Module):
  def __init__(self):
    super( DeepConvNet_Relu, self).__init__()
    self.firstConv = nn.Sequential(
        nn.Conv2d(1, 25, kernel_size=(1,5), stride=1, padding=(0,25) , bias=False),
        nn.Conv2d(25, 25, kernel_size=(2,1), stride=1, padding=(0,25) , bias=False),
        nn.BatchNorm2d(25),
        nn.ReLU(),
        nn.MaxPool2d(kernel_size=(1,2)),
        nn.Dropout(p=0.5)
    )
    self.secondConv = nn.Sequential(
        nn.Conv2d(25, 50, kernel_size=(1,5)),
        nn.BatchNorm2d(50),
        nn.ReLU(),
        nn.MaxPool2d(kernel_size=(1,2)),
        nn.Dropout(p=0.5)
    )
    self.thirdConv = nn.Sequential(
        nn.Conv2d(50, 100, kernel_size=(1,5)),
        nn.BatchNorm2d(100),
        nn.ReLU(),
        nn.MaxPool2d(kernel_size=(1,2)),
        nn.Dropout(p=0.5)
    )
    self.fourthConv = nn.Sequential(
        nn.Conv2d(100, 200, kernel_size=(1,5)),
        nn.BatchNorm2d(200),
        nn.ReLU(),
        nn.MaxPool2d(kernel_size=(1,2)),
        nn.Dropout(p=0.5)
    )
    self.classify = nn.Sequential(
        nn.Linear(9800, 2, bias=True)
    )

  def forward(self, x):
    x = self.firstConv(x)
    x = self.secondConv(x)
    x = self.thirdConv(x)
    x = self.fourthConv(x)
    x = x.flatten(1)
    x = self.classify(x)
    return x

class DeepConvNet_LeakyReLU(nn.Module):
  def __init__(self):
    super( DeepConvNet_LeakyReLU, self).__init__()
    self.firstConv = nn.Sequential(
        nn.Conv2d(1, 25, kernel_size=(1,5), stride=1, padding=(0,25) , bias=False),
        nn.Conv2d(25, 25, kernel_size=(2,1), stride=1, padding=(0,25) , bias=False),
        nn.BatchNorm2d(25),
        nn.LeakyReLU(),
        nn.MaxPool2d(kernel_size=(1,2)),
        nn.Dropout(p=0.5)
    )
    self.secondConv = nn.Sequential(
        nn.Conv2d(25, 50, kernel_size=(1,5)),
        nn.BatchNorm2d(50),
        nn.LeakyReLU(),
        nn.MaxPool2d(kernel_size=(1,2)),
        nn.Dropout(p=0.5)
    )
    self.thirdConv = nn.Sequential(
        nn.Conv2d(50, 100, kernel_size=(1,5)),
        nn.BatchNorm2d(100),
        nn.LeakyReLU(),
        nn.MaxPool2d(kernel_size=(1,2)),
        nn.Dropout(p=0.5)
    )
    self.fourthConv = nn.Sequential(
        nn.Conv2d(100, 200, kernel_size=(1,5)),
        nn.BatchNorm2d(200),
        nn.LeakyReLU(),
        nn.MaxPool2d(kernel_size=(1,2)),
        nn.Dropout(p=0.5)
    )
    self.classify = nn.Sequential(
        nn.Linear(9800, 2, bias=True)
    )

  def forward(self, x):
    x = self.firstConv(x)
    x = self.secondConv(x)
    x = self.thirdConv(x)
    x = self.fourthConv(x)
    x = x.flatten(1)
    x = self.classify(x)
    return x

class DeepConvNet_ELU(nn.Module):
  def __init__(self):
    super( DeepConvNet_ELU, self).__init__()
    self.firstConv = nn.Sequential(
        nn.Conv2d(1, 25, kernel_size=(1,5), stride=1, padding=(0,25) , bias=False),
        nn.Conv2d(25, 25, kernel_size=(2,1), stride=1, padding=(0,25) , bias=False),
        nn.BatchNorm2d(25),
        nn.ELU(),
        nn.MaxPool2d(kernel_size=(1,2)),
        nn.Dropout(p=0.5)
    )
    self.secondConv = nn.Sequential(
        nn.Conv2d(25, 50, kernel_size=(1,5)),
        nn.BatchNorm2d(50),
        nn.ELU(),
        nn.MaxPool2d(kernel_size=(1,2)),
        nn.Dropout(p=0.5)
    )
    self.thirdConv = nn.Sequential(
        nn.Conv2d(50, 100, kernel_size=(1,5)),
        nn.BatchNorm2d(100),
        nn.ELU(),
        nn.MaxPool2d(kernel_size=(1,2)),
        nn.Dropout(p=0.5)
    )
    self.fourthConv = nn.Sequential(
        nn.Conv2d(100, 200, kernel_size=(1,5)),
        nn.BatchNorm2d(200),
        nn.ELU(),
        nn.MaxPool2d(kernel_size=(1,2)),
        nn.Dropout(p=0.5)
    )
    self.classify = nn.Sequential(
        nn.Linear(9800, 2, bias=True)
    )

  def forward(self, x):
    x = self.firstConv(x)
    x = self.secondConv(x)
    x = self.thirdConv(x)
    x = self.fourthConv(x)
    x = x.flatten(1)
    x = self.classify(x)
    return x
DeepConvNet_ELU = DeepConvNet_ELU()    
print(DeepConvNet_ELU)

def train(model,n_epochs,activation):
  
  writer = SummaryWriter()
  # "cuda" only when GPUs are available.
  device = "cuda" if torch.cuda.is_available() else "cpu"

  print("now devoce is",device)
  model = model.to(device)
  model.device = device
  print(model)

  # For the classification task, we use cross-entropy as the measurement of performance.
  criterion = nn.CrossEntropyLoss()

  # Initialize optimizer
  if activation == "SGDM":
    optimizer = torch.optim.SGD(model.parameters(), lr=1e-3, weight_decay=1e-5, momentum=0.9)
  elif activation == "RMSprop":
    optimizer = torch.optim.RMSprop(model.parameters())
  else: 
    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-5)

  for epoch in range(n_epochs):
    # ---------- Training ----------
      # Make sure the model is in train mode before training.
      model.train()

      # These are used to record information in training.
      train_loss = []
      train_accs = []

      # Iterate the training set by batches.
      for batch in train_loader: # or in tqdm(test_loader):

          # A batch consists of image data and corresponding labels.
          imgs, labels = batch
          # Forward the data. (Make sure data and model are on the same device.)
          logits = model(imgs.to(device))

          # Calculate the cross-entropy loss.
          loss = criterion(logits, labels.to(device))

          # Gradients stored in the parameters in the previous step should be cleared out first.
          optimizer.zero_grad()

          # Compute the gradients for parameters.
          loss.backward()

          # Clip the gradient norms for stable training.
          grad_norm = nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)

          # Update the parameters with computed gradients.
          optimizer.step()

          # Compute the accuracy for current batch.
          acc = (logits.argmax(dim=-1) == labels.to(device)).float().mean()

          # Record the loss and accuracy.
          train_loss.append(loss.item())
          train_accs.append(acc)

      # The average loss and accuracy of the training set is the average of the recorded values.
      train_loss = sum(train_loss) / len(train_loss)
      train_acc = sum(train_accs) / len(train_accs)

      writer.add_scalar('Loss/train', train_loss, epoch)
      writer.add_scalar('Accuracy/train', train_acc, epoch)

      # Print the information.
      print(f"[ Train | {epoch + 1:03d}/{n_epochs:03d} ] loss = {train_loss:.5f}, acc = {train_acc:.5f}")

    # ---------- Validation ----------
    # Make sure the model is in eval mode so that some modules like dropout are disabled and work normally.
    
      model.eval()

      # These are used to record information in validation.
      valid_loss = []
      valid_accs = []

      # Iterate the validation set by batches.
      for batch in valid_loader: # or in tqdm(test_loader):

          # A batch consists of image data and corresponding labels.
          imgs, labels = batch

          # We don't need gradient in validation.
          # Using torch.no_grad() accelerates the forward process.
          with torch.no_grad():
            logits = model(imgs.to(device))

          # We can still compute the loss (but not the gradient).
          loss = criterion(logits, labels.to(device))

          # Compute the accuracy for current batch.
          acc = (logits.argmax(dim=-1) == labels.to(device)).float().mean()

          # Record the loss and accuracy.
          valid_loss.append(loss.item())
          valid_accs.append(acc)

      # The average loss and accuracy for entire validation set is the average of the recorded values.
      valid_loss = sum(valid_loss) / len(valid_loss)
      valid_acc = sum(valid_accs) / len(valid_accs)

      writer.add_scalar('Loss/valid', valid_loss, epoch)        
      writer.add_scalar('Accuracy/valid', valid_acc, epoch)

      # Print the information.
      print(f"[ Valid | {epoch + 1:03d}/{n_epochs:03d} ] loss = {valid_loss:.5f}, acc = {valid_acc:.5f}")


  writer.close()

# Make sure the model is in eval mode.
# Some modules like Dropout or BatchNorm affect if the model is in training mode.
def test(model):
  device = "cuda" if torch.cuda.is_available() else "cpu"
  model.eval()
  # Initialize a list to store the predictions.
  test_accs = []
  # Iterate the testing set by batches.
  for batch in test_loader: # or in tqdm(test_loader):

      imgs, labels = batch

      # Using torch.no_grad() accelerates the forward process.
      with torch.no_grad():
          logits = model(imgs.to(device))
    
      # Compute the accuracy for current batch.
          acc = (logits.argmax(dim=-1) == labels.to(device)).float().mean()

          # Record the loss and accuracy.
          test_accs.append(acc)

  test_acc = sum(test_accs) / len(test_accs)

  print(f"test_set acc = {test_acc:.5f}")
  return test_acc

def save_model(model,model_name):
  FILE = str(model_name) + ".pth"
  torch.save(model.state_dict(), FILE)


def load_model(model,model_name):
  device = "cuda" if torch.cuda.is_available() else "cpu"
  FILE = str(model_name) + ".pth"
  model.load_state_dict(torch.load(FILE))
  model.to(device)
  model.eval()

'''# Initialize a model, and put it on the device specified.
#train(model, n_epochs, batch_size)
DeepConvNet_Relu = DeepConvNet_Relu()
train(DeepConvNet_Relu,300,"Adam")
DeepConvNet_Relu_acc = test(DeepConvNet_Relu)'''

'''# Initialize a model, and put it on the device specified.
#train(model, n_epochs, batch_size)
DeepConvNet_LeakyReLU = DeepConvNet_LeakyReLU()
train(DeepConvNet_LeakyReLU,300,"Adam")
DeepConvNet_LeakyReLU_acc = test(DeepConvNet_LeakyReLU)'''

'''# Initialize a model, and put it on the device specified.
#train(model, n_epochs, batch_size)
DeepConvNet_ELU = DeepConvNet_ELU()
train(DeepConvNet_ELU,300,"Adam")
DeepConvNet_ELU_acc = test(DeepConvNet_ELU)'''

'''# Initialize a model, and put it on the device specified.
#train(model, n_epochs, batch_size)
EGGNet_ReLU = EGGNet_ReLU()
train(EGGNet_ReLU,300,"Adam")
EGGNet_ReLU_acc = test(EGGNet_ReLU)'''

'''# Initialize a model, and put it on the device specified.
#train(model, n_epochs, batch_size)
EGGNet_LeakyReLU = EGGNet_LeakyReLU()
train(EGGNet_LeakyReLU,300,"Adam")
EGGNet_LeakyReLU_acc = test(EGGNet_LeakyReLU)'''

'''# Initialize a model, and put it on the device specified.
#train(model, n_epochs, batch_size)
EGGNet_ELU = EGGNet_ELU()
train(EGGNet_ELU,300,"Adam")
EGGNet_ELU_acc = test(EGGNet_ELU)'''

'''print(f"DeepConvNet_Relu_acc = {DeepConvNet_Relu_acc:.5f}")
print(f"DeepConvNet_LeakyReLU_acc = {DeepConvNet_LeakyReLU_acc:.5f}")
print(f"DeepConvNet_ELU_acc = {DeepConvNet_ELU_acc:.5f}")'''
'''print(f"EGGNet_ReLU_acc = {EGGNet_ReLU_acc:.5f}")
print(f"EGGNet_LeakyReLU_acc = {EGGNet_LeakyReLU_acc:.5f}")
print(f"EGGNet_ELU_acc = {EGGNet_ELU_acc:.5f}")'''

'''test(DeepConvNet_ELU)
save_model(DeepConvNet_ELU,'DeepConvNet_ELU')'''

DeepConvNet_Relu_DEMO = DeepConvNet_Relu()
load_model(DeepConvNet_Relu_DEMO,'DeepConvNet_Relu')
print('DeepConvNet_Relu:')
test(DeepConvNet_Relu_DEMO)#batch 1024

DeepConvNet_LeakyReLU_DEMO = DeepConvNet_LeakyReLU()
load_model(DeepConvNet_LeakyReLU_DEMO,'DeepConvNet_LeakyReLU')
print('DeepConvNet_LeakyReLU:')
test(DeepConvNet_LeakyReLU_DEMO)#batch 1024

DeepConvNet_ELU_DEMO = DeepConvNet_ELU()
load_model(DeepConvNet_ELU_DEMO,'DeepConvNet_ELU')
print('DeepConvNet_ELU:')
test(DeepConvNet_ELU_DEMO)#batch 1024

EGGNet_ReLU_DEMO = EGGNet_ReLU()
load_model(EGGNet_ReLU_DEMO,'EGGNet_ReLU')
print('EGGNet_ReLU:')
test(EGGNet_ReLU_DEMO)#batch 1024

EGGNet_LeakyReLU_DEMO = EGGNet_LeakyReLU()
load_model(EGGNet_LeakyReLU_DEMO,'EGGNet_LeakyReLU')
print('EGGNet_LeakyReLU:')
test(EGGNet_LeakyReLU_DEMO)#batch 1024

EGGNet_ELU_DEMO = EGGNet_ELU()
load_model(EGGNet_ELU_DEMO,'EGGNet_ELU')
print('EGGNet_ELU:')
test(EGGNet_ELU_DEMO)#batch 1024



#!kill 3668